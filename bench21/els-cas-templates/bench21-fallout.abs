All modern distributed systems list performance and scalability as their core strengths. Given that
optimal performance requires carefully selecting configuration options, and typical cluster
sizes can range anywhere from 2 to 300 nodes, itâ€™s rare for any two clusters to be exactly
the same. Validating the behavior and performance of distributed systems in this large
configuration space is challenging without automation that stretches across the software
stack. Additionally, the diverse set of benchmarking tools available means many engineering
teams have their own preference for the tools they use for performance testing.

In this paper we present Fallout, an open-source distributed systems testing service that
automatically provisions and configures distributed systems and clients, supports running a
variety of workloads and benchmarks, and generates performance reports based on collected
metrics for visual analysis. We will specifically cover how Fallout is being used to test
Apache Cassandra and Pulsar. Fallout test runs are specified using plain-text YAML
configuration files which are amenable to storing in version control and encourage test
configuration reuse and historical documentation. We have been running the Fallout service
internally at DataStax for over 5 years and have recently open sourced it to support our
work with Apache Cassandra, Pulsar, and other open source projects. We have used Fallout to
identify performance bottlenecks, validate new optimizations under various loads at various
cluster scales, and recreate customer issues to aid debugging. We describe the architecture
of Fallout along with the evolution of its design and the lessons we learned operating this
service in a dynamic environment where teams work on different products and favor different
benchmarking tools.
